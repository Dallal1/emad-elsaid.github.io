* [Paper](https://arxiv.org/pdf/1706.03762.pdf)

# Terms 
* Long short-term memory
* Attention
  > In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
* Extended Neural GPU

# Diagrams

![](/public/11fc2e73edd0020b8b93563ec28e53d597ec2e0b671658e462f62cac577b9857.png)